To run the preprocessing code you will need to install the [body keypoints](https://drive.google.com/file/d/1k5mfrc2g4ZEzzNjW6CEVjLvNTZcmPanB/view?usp=sharing).
First run the video downloader file, and make sure that you are using the asl20.json or just use the dataset located [here](https://virginiatech-my.sharepoint.com/:u:/g/personal/jboyle1013_vt_edu/EVUHfdyBg3FDgUYSY7NkVbABdgDRMPc4Ira-lkRmUEyWnQ?e=jxgbi0).
If you downloaded the dataset above you can skip this step, as you have everything you need to train.
Then run the preprocessing, either cleaned or 300 in a unix environment, a bash script is used to make sure that all of the raw video files are mp4s.
After retreiving the dataset, go to the Preprocess directory and run prune.py with the data_dir variable changed to your dataset directory name and pick a name for your output directory paths. prune.py will resave the data set but with frames that don't have any hands in view. Use the output directory name you chose as the new data_dir input in both mediapipe_preprocess_get_features.py and preprocess_get_labels.py. Run these two files to get labels and features which are saved as npy files in the new data_dir you specified. Then run cnn.py which is found in the CNN folder. 
